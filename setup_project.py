#!/usr/bin/env python3
"""
Setup script to create project directory structure and verify installation

"""

import os
import sys
import subprocess
from pathlib import Path
import shutil

def create_directory_structure():
    """Create all necessary directories for the project"""
    directories = [
        "data",
        "models/baseline",
        "models/neural",
        "models/saved",
        "evaluation",
        "app",
        "processed_data",
        "results",
        "plots",
        "notebooks"
    ]
    
    print("Creating project directory structure...")
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        print(f"✓ Created {directory}/")
    
    # Create __init__.py files for Python packages
    init_files = [
        "data/__init__.py",
        "models/__init__.py",
        "models/baseline/__init__.py",
        "models/neural/__init__.py",
        "evaluation/__init__.py",
        "app/__init__.py"
    ]
    
    for init_file in init_files:
        Path(init_file).touch(exist_ok=True)
    
    print("\n✅ Directory structure created successfully!")

def create_python_files():
    """Create Python files if they don't exist"""
    print("\nCreating Python module files...")
    
    # Check if files already exist as notebooks and handle accordingly
    files_to_create = {
        'data/preprocessing.py': 'preprocessing',
        'data/augmentation.py': 'augmentation',
        'models/baseline/logistic_regression.py': 'logistic_regression',
        'models/baseline/naive_bayes.py': 'naive_bayes',
        'models/baseline/svm.py': 'svm',
        'models/neural/lstm.py': 'lstm',
        'models/neural/bert_classifier.py': 'bert_classifier',
        'models/neural/response_generator.py': 'response_generator',
        'evaluation/metrics.py': 'metrics',
        'evaluation/visualizations.py': 'visualizations',
        'app/streamlit_demo.py': 'streamlit_demo'
    }
    
    # Create minimal working files if they don't exist
    for filepath, module_name in files_to_create.items():
        if not os.path.exists(filepath):
            print(f"Creating {filepath}...")
            
            # Create a minimal working module
            content = f'''"""
{module_name.replace('_', ' ').title()} Module
Auto-generated by setup_project.py
"""

import warnings
warnings.filterwarnings('ignore')

# Add your implementation here
print(f"Module {module_name} loaded successfully!")
'''
            
            with open(filepath, 'w') as f:
                f.write(content)
            print(f"✓ Created {filepath}")
        else:
            print(f"✓ {filepath} already exists")

def move_processed_data():
    """Move processed data files to correct location if needed"""
    # Check if processed data exists in wrong locations
    data_files = ['amazon_processed.csv', 'twitter_processed.csv', 'conversation_pairs.csv']
    
    for filename in data_files:
        # Check in data/ directory
        if os.path.exists(f'data/{filename}') and not os.path.exists(f'processed_data/{filename}'):
            shutil.move(f'data/{filename}', f'processed_data/{filename}')
            print(f"✓ Moved {filename} to processed_data/")

def check_dependencies():
    """Check if all required dependencies are installed"""
    print("\nChecking dependencies...")
    
    required_packages = [
        ("numpy", "numpy"),
        ("pandas", "pandas"),
        ("sklearn", "scikit-learn"),
        ("nltk", "nltk"),
        ("tensorflow", "tensorflow"),
        ("torch", "torch"),
        ("transformers", "transformers"),
        ("streamlit", "streamlit"),
        ("plotly", "plotly"),
        ("seaborn", "seaborn"),
        ("matplotlib", "matplotlib"),
        ("tqdm", "tqdm"),
        ("wordcloud", "wordcloud"),
        ("joblib", "joblib")
    ]
    
    missing_packages = []
    
    for import_name, package_name in required_packages:
        try:
            __import__(import_name)
            print(f"✓ {package_name}")
        except ImportError:
            print(f"✗ {package_name} (missing)")
            missing_packages.append(package_name)
    
    if missing_packages:
        print(f"\n⚠️  Missing packages: {', '.join(missing_packages)}")
        print("\nInstall missing packages with:")
        print(f"pip install {' '.join(missing_packages)}")
        return False
    else:
        print("\n✅ All dependencies installed!")
        return True

def download_nltk_data():
    """Download required NLTK data"""
    print("\nDownloading NLTK data...")
    
    try:
        import nltk
        
        nltk_data = [
            'punkt',
            'punkt_tab',
            'stopwords',
            'wordnet',
            'averaged_perceptron_tagger',
            'omw-1.4'
        ]
        
        for data in nltk_data:
            try:
                nltk.download(data, quiet=True)
                print(f"✓ {data}")
            except:
                print(f"⚠️  Failed to download {data}")
        
        print("\n✅ NLTK data downloaded!")
        return True
        
    except ImportError:
        print("⚠️  NLTK not installed. Install dependencies first.")
        return False

def check_datasets():
    """Check if datasets are present"""
    print("\nChecking for datasets...")
    
    datasets = {
        "Reviews.csv": "Amazon Fine Food Reviews",
        "twcs.csv": "Twitter Customer Support"
    }
    
    missing_datasets = []
    
    for filename, name in datasets.items():
        if os.path.exists(filename):
            size_mb = os.path.getsize(filename) / (1024 * 1024)
            print(f"✓ {filename} ({name}) - {size_mb:.1f} MB")
        else:
            print(f"✗ {filename} ({name}) - Not found")
            missing_datasets.append((filename, name))
    
    # Also check for processed data
    print("\nChecking for processed data...")
    processed_files = ['amazon_processed.csv', 'twitter_processed.csv', 'conversation_pairs.csv']
    processed_found = 0
    
    for filename in processed_files:
        if os.path.exists(f'processed_data/{filename}'):
            print(f"✓ processed_data/{filename}")
            processed_found += 1
        else:
            print(f"✗ processed_data/{filename} - Not found")
    
    if missing_datasets:
        print("\n⚠️  Missing datasets:")
        print("Please download from:")
        print("- Amazon Reviews: https://www.kaggle.com/snap/amazon-fine-food-reviews")
        print("- Twitter Support: https://www.kaggle.com/thoughtvector/customer-support-on-twitter")
        return False
    else:
        print("\n✅ All datasets found!")
        if processed_found == len(processed_files):
            print("✅ All processed data files found!")
        return True

def create_sample_notebook():
    """Create a sample working notebook"""
    notebook_content = '''# Customer Support Sentiment Analysis - Working Notebook

## 1. Load Processed Data

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Load processed data
amazon_df = pd.read_csv('processed_data/amazon_processed.csv')
print(f"Loaded {len(amazon_df)} reviews")
```

## 2. Prepare Data

```python
X = amazon_df['processed_text'].values
y = amazon_df['binary_sentiment'].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

## 3. Train Model

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000)),
    ('classifier', LogisticRegression())
])

pipeline.fit(X_train, y_train)
```

## 4. Evaluate

```python
y_pred = pipeline.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(classification_report(y_test, y_pred))
```
'''
    
    with open('working_notebook.md', 'w') as f:
        f.write(notebook_content)
    print("\n✅ Created working_notebook.md with examples")

def fix_imports_for_notebooks():
    """Create a helper file for notebook imports"""
    helper_content = '''"""
Import helper for Jupyter notebooks
Add this to your notebook: from notebook_imports import *
"""

import sys
import os

# Add parent directory to path
sys.path.append('..')
sys.path.append('.')

# Common imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

print("✅ All imports loaded successfully!")

# Helper functions
def load_processed_data():
    """Load all processed datasets"""
    amazon_df = pd.read_csv('processed_data/amazon_processed.csv')
    twitter_df = pd.read_csv('processed_data/twitter_processed.csv')
    conv_pairs = pd.read_csv('processed_data/conversation_pairs.csv')
    
    print(f"Loaded {len(amazon_df)} Amazon reviews")
    print(f"Loaded {len(twitter_df)} Twitter messages")
    print(f"Loaded {len(conv_pairs)} conversation pairs")
    
    return amazon_df, twitter_df, conv_pairs

def quick_train_model(X_train, y_train, X_test, y_test, model_type='lr'):
    """Quick model training helper"""
    models = {
        'lr': LogisticRegression(max_iter=1000),
        'nb': MultinomialNB(),
        'svm': LinearSVC(max_iter=1000)
    }
    
    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(max_features=5000)),
        ('classifier', models[model_type])
    ])
    
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"{model_type.upper()} Accuracy: {accuracy:.4f}")
    return pipeline, y_pred
'''
    
    with open('notebook_imports.py', 'w') as f:
        f.write(helper_content)
    print("✅ Created notebook_imports.py helper file")

def main():
    """Main setup function"""
    print("="*60)
    print("Customer Support Sentiment Analysis - Project Setup")
    print("="*60)
    
    # Get current directory
    print(f"\nCurrent directory: {os.getcwd()}")
    
    # Create directory structure
    create_directory_structure()
    
    # Create Python files
    create_python_files()
    
    # Move processed data if needed
    move_processed_data()
    
    # Create import helper
    fix_imports_for_notebooks()
    
    # Check dependencies
    deps_ok = check_dependencies()
    
    # Download NLTK data if dependencies are installed
    if deps_ok:
        download_nltk_data()
    
    # Check datasets
    datasets_ok = check_datasets()
    
    # Create sample notebook
    create_sample_notebook()
    
    print("\n" + "="*60)
    
    if deps_ok:
        print(" Setup complete! You're ready to start.")
        print("\n Quick Start for Jupyter Notebooks:")
        print("1. Use existing processed data: pd.read_csv('processed_data/amazon_processed.csv')")
        print("2. Import helper: from notebook_imports import *")
        print("3. Or run: amazon_df, twitter_df, conv_pairs = load_processed_data()")
        print("\n Next steps:")
        print("- Open any Jupyter notebook and start coding")
        print("- Use working_notebook.md for examples")
        print("- Run models with the processed data")
    else:
        print("  Setup incomplete. Please:")
        if not deps_ok:
            print("- Install missing dependencies")
        print("\n Quick install all dependencies:")
        print("pip install pandas numpy scikit-learn matplotlib seaborn nltk torch transformers streamlit plotly tqdm wordcloud joblib")
    
    print("="*60)

if __name__ == "__main__":
    main()